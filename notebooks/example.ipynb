{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Analysis with OptLevAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "Import required packages along with `AggregateData` and `FileData` from `../lib/data_processing.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('../scripts/optlevstyle.mplstyle')\n",
    "from data_processing import AggregateData, FileData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a single file\n",
    "We can use the `FileData` class directly to load and process data from a single file. Start by creating the `FileData` object, then run the `load_data()` method to populate the object attributes with the raw data.\n",
    "\n",
    "This method calls the following:\n",
    "* `read_hdf5()` to create a dictionary of the raw data\n",
    "* `get_laser_power()` to get the laser power and transmitted power\n",
    "* `get_diag_mat()` to extract the QPD diagonalization matrix from the config file, if the argument `diagonalize_qpd` is `True`.\n",
    "* `get_xyz_from_quad()` to calculate the x, y, and z data from the QPD data, which in turn calls\n",
    "    * `extract_quad()` to parse the raw QPD data into sensible arrays\n",
    "* `calibrate_stage_position()` to get the cantilever positions in microns\n",
    "* `calibrate_bead_response()` to apply the transfer function for both the QPD and PSPD or DC QPD. This calls one of the following:\n",
    "    * `tf_array_fitted()` to get the transfer function based on fitting the poles/zeros, used for all data from 2023 and later\n",
    "    * `tf_array_interpolated()` to interpolate the transfer function at the desired frequencies. Only included to reprocess Wilson data with the old transfer function format\n",
    "* `get_boolean_cant_filter()` to create a filter of a specified width for selecting only the harmonics used later in the analysis\n",
    "* `get_ffts_and_noise()` to get the FFTs for the cantilever and the bead, along with the noise from the sidebands\n",
    "* `make_templates()` to use the bead and cantilever data to compute a signal template given a signal model.\n",
    "\n",
    "More information on how to use each method can be found in the comments in the data processing script. After loading the data, the object should have the attributes listed after the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerometer\n",
      "bead_height\n",
      "build_drive_mask\n",
      "calibrate_bead_response\n",
      "calibrate_stage_position\n",
      "camera_status\n",
      "cant_fft\n",
      "cant_inds\n",
      "cant_pos_calibrated\n",
      "cant_raw_data\n",
      "data_dict\n",
      "date\n",
      "diagonalize_qpd\n",
      "drive_ind\n",
      "drop_raw_data\n",
      "error_log\n",
      "extract_quad\n",
      "file_name\n",
      "filter_raw_data\n",
      "force_cal_factors_qpd\n",
      "force_cal_factors_xypd\n",
      "freqs\n",
      "fsamp\n",
      "fund_ind\n",
      "get_boolean_cant_mask\n",
      "get_ffts_and_noise\n",
      "get_laser_power\n",
      "get_motion_likeness\n",
      "get_xyz_from_quad\n",
      "good_inds\n",
      "is_bad\n",
      "laser_power_full\n",
      "load_data\n",
      "make_templates\n",
      "mean_cant_pos\n",
      "mean_laser_power\n",
      "mean_p_trans\n",
      "motion_likeness\n",
      "nsamp\n",
      "p_trans_full\n",
      "qpd_diag_mat\n",
      "qpd_ffts\n",
      "qpd_ffts_full\n",
      "qpd_force_calibrated\n",
      "qpd_sb_ffts\n",
      "quad_amps\n",
      "quad_null\n",
      "quad_phases\n",
      "quad_raw_data\n",
      "read_hdf5\n",
      "template_ffts\n",
      "template_params\n",
      "tf_array_fitted\n",
      "tf_array_interpolated\n",
      "times\n",
      "window\n",
      "window_s1\n",
      "window_s2\n",
      "xypd_ffts\n",
      "xypd_ffts_full\n",
      "xypd_force_calibrated\n",
      "xypd_raw_data\n",
      "xypd_sb_ffts\n"
     ]
    }
   ],
   "source": [
    "filedat = FileData('/data/new_trap/20231009/Bead0/Gravity/1/shaking_1.h5')\n",
    "filedat.load_data()\n",
    "# print out the class attributes\n",
    "for d in dir(filedat):\n",
    "    if d[:2]!='__':\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling multiple files or datasets\n",
    "The `FileData` class is intended to be called by the `AggregateData` class when loading multiple files at once.\n",
    "\n",
    "The `AggregateData` class is designed to hold data of two types:\n",
    "1. The data from each individual `FileData` object that may be needed for analysis. These data are stored in a dictionary called `agg_dict`. For each key in the dictionary, there will be a numpy array whose first dimension is the number of files loaded by the `AggregateData` object.\n",
    "2. Information pertaining to subsets of the data in the `AggregateData` object rather than an individual file. These will be stored as class attributes. The arrays may have different first dimensions, since some of the datasets loaded into the `AggregateData` object may share certain parameters. For example, the attribute `diam_bead` may be an array with one entry if all files loaded used the same size bead, while the attribute `p0_bead` may have multiple entries corresponding to different subsets of the data collected with the bead at different positions relative to the attractor. The `bin_indices` attribute will allow for subsets of the data to be selected based on any of these parameters, as described in a later cell.\n",
    "\n",
    "To initialize an `AggregateData` object, call `AggregateData()` with the following arguments:\n",
    "* `data_dirs`, a list of the full paths to the directories containing the raw hdf5 files to be processed\n",
    "* `file_prefixes`, a list of prefixes used to select specific files from within the directory. If you want all files from a directory, omit this argument. If you want multiple prefixes from the same directory, repeat the directory path in the `data_dirs` argument and add the corresponding file prefixes.\n",
    "* `descrips`, a list of descriptions intended to be useful to the user in identifying different dataset\n",
    "* `num_to_load` a list of the number of files to load from each directory, or an integer to load the same number from all directories. If this argument is omitted, all files from each directory will be included.\n",
    "\n",
    "In the example below, an `AggregateData` object for 100 files from the Wilson data and a dataset from June 14, 2023 is initialized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggdat = AggregateData(['/data/new_trap/20231009/Bead0/Gravity/1/','/data/new_trap/20200320/Bead1/Shaking/Shaking378/'],\\\n",
    "                       file_prefixes=['shaking_','Shaking1'],descrips=['Oct 2023','Wilson'],num_to_load=[100,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "To load and preprocess the data, call the `load_file_data()` method with the following arguments:\n",
    "* `num_cores`, the number of cores to use for processing the files in parallel\n",
    "* `diagonalize_qpd`, whether to look in the config file for a diagonalization matrix to use for the calculation of $x$ and $y$ from the QPD.\n",
    "* `load_templates`, whether to load the signal templates for each file.\n",
    "* `harms`, a list of harmonics to be used in the analysis.\n",
    "* `no_tf` and `no_config`, whether to load the file data without looking for a config file or transfer function data. If `True`, default parameters will be used.\n",
    "* `lightweight`, a boolean parameter which determines whether or not to keep the raw data as a class attribute. If set to False, only the reduced data will be accessible within the `AggregateData` object.\n",
    "\n",
    "This method also takes care of a few additional organizational steps:\n",
    "* Saves the full list of files imported as a class attribute\n",
    "* Loads the bead positions `p0_bead` and diameters `diam_bead` from a config file, `config.yaml`, located in the same directory as the hdf5 files. Removes any duplicate entries for easier indexing (described in more detail further down).\n",
    "* Identifies any bad files, saves a list of the filenames, and purges them from the `AggregateData` object\n",
    "\n",
    "At this point a list of the `FileData` objects has been loaded, so all the `FileData` attributes can be accessed, albeit in a clunky way. To make accessing these parameters for many datasets at a time easier, a private methoc called `__build_dict()` will extract all the data from the `FileData` objects and put it into the `agg_dict` attribute of the `AggregateData` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 200 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 31.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 4 files could not be loaded.\n",
      "Successfully loaded 196 files.\n",
      "Building dictionary of file data...\n",
      "Done building dictionary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'fsamp', 'nsamp', 'window_s1', 'window_s2', 'fund_ind', 'good_inds', 'times', 'timestamp', 'accelerometer', 'bead_height', 'mean_laser_power', 'laser_power_full', 'mean_p_trans', 'p_trans_full', 'cant_raw_data', 'quad_raw_data', 'quad_null', 'mean_cant_pos', 'qpd_ffts', 'qpd_ffts_full', 'qpd_sb_ffts', 'xypd_ffts', 'xypd_ffts_full', 'xypd_sb_ffts', 'template_ffts', 'template_params', 'cant_fft', 'quad_amps', 'quad_phases', 'mot_likes', 'axis_angles', 'camera_status'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggdat.load_file_data(num_cores=20,lightweight=False,downsample=False)\n",
    "aggdat.agg_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning by auxiliary data\n",
    "Ultimately we may want to load many datasets but look only at a subset of that data that meets some criteria for a particular analysis. Rather than creating multiple sparse, nested arrays, data for all files loaded is kept in a single 1-D array, and a secondary array is used to track the indices. For each file loaded, there is a corresponding row in the array of indices that includes the following information:\n",
    "* `diam_bead` index: The index that can be used to select the correct bead diameter value from the array containing all values for the `AggregateData` object.\n",
    "* `p0_bead` index: Same thing, but for the bead position.\n",
    "* `descrips` index: Same thing, but for the descriptions passed in for each directory when the constructor was called.\n",
    "* `cant_bins_x` index: An index that determines which bin of mean cantilever x positions the file falls into.\n",
    "* `cant_bins_z` index: Same thing, but for the mean cantilever z positions.\n",
    "* `seis_thresh` index: 0 if the file passes the seismometer threshold cut, 1 if it doesn't.\n",
    "* `bias` index: Not yet implemented, but will eventualy index the attractor/shield bias.\n",
    "* `qpd_asds` and `xypd_asds` indices (the same for both): Index of the RMS spectral densities for that subset of the data.\n",
    "* `is_bad` index: 1 if the file is bad, 0 otherwise. This is used for purging, so in principle after the data is loaded all values should be 0.\n",
    "\n",
    "The method `bin_by_aux_data()` takes the following arguments:\n",
    "* `cant_bin_widths`: a list containing the bin widths for the cantilever x and z positions. The data will be histogrammed into bins of the specified widths, then empty bins will be removed and the indices adjusted accordingly.\n",
    "* `seis_thresh`: the threshold value used in the seismometer data cut.\n",
    "\n",
    "Once this method has been called, the `bin_indices` attribute should be populated with indices pertaining to the auxiliary data, and can be used for indexing arbitrary slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning data by mean cantilever position and accelerometer data...\n",
      "Done binning data.\n",
      "[[0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[1 0 1 1 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 1 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "aggdat.bin_by_aux_data(cant_bin_widths=[1.,1.],accel_thresh=0.01)\n",
    "print(aggdat.bin_indices[:5])\n",
    "print(aggdat.bin_indices[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the RMS spectral densities for large datasets\n",
    "With the full FFT data for each file loaded, the `AggregateData` objects can become quite large. Since the individual FFTs for each dataset may not be needed, the method `estimate_spectral_densities()` can be used to compute the RMS average of the amplitude spectral densities for each combination of run conditions. This will add class attributes `qpd_asds` and `xypd_asds`, arrays whose first dimension represents the subset of the data to which the spectra correspond, second dimension represents the axis, and third dimension represents the frequency. Corresponding indices will be added to the `bin_indices` attribute so that it can be easily determined which file contributes to which RMS spectral density.\n",
    "\n",
    "Once this has been run, the method `drop_full_ffts()` can be used to delete the full FFT data for each file and release memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating RMS amplitude spectral density for each set of run conditions...\n",
      "Amplitude spectral densities estimated for 3 sets of run conditions.\n",
      "AggregateData spectral density shapes:\n",
      "(3, 3, 5001)\n",
      "(3, 2, 5001)\n",
      "Dictionary column shapes before dropping FFT data:\n",
      "(196, 4, 5001)\n",
      "(196, 3, 5001)\n",
      "After dropping FFT data:\n",
      "(196, 4, 1)\n",
      "(196, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "aggdat.estimate_spectral_densities()\n",
    "print('AggregateData spectral density shapes:')\n",
    "print(aggdat.qpd_asds.shape)\n",
    "print(aggdat.xypd_asds.shape)\n",
    "print('Dictionary column shapes before dropping FFT data:')\n",
    "print(aggdat.agg_dict['qpd_ffts_full'].shape)\n",
    "print(aggdat.agg_dict['xypd_ffts_full'].shape)\n",
    "aggdat.drop_full_ffts()\n",
    "print('After dropping FFT data:')\n",
    "print(aggdat.agg_dict['qpd_ffts_full'].shape)\n",
    "print(aggdat.agg_dict['xypd_ffts_full'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing data with auxiliary parameters\n",
    "\n",
    "The config data and auxiliary parameters are not saved in `agg_dict` to avoid saving the same values many times over and wasting memory. However, it may be convenient to get arrays of these parameters for use in masking/indexing the data in `agg_dict`. The method `get_parameter_arrays()` will return a tuple of arrays of all the config data and auxiliary parameters. The arrays will have the same first dimension as the data in `agg_dict` so that they can be used in indexing. The method will also print out the order in which the arrays are returned. The cell below shows an example of how the `mean_laser_power` data can be masked to select only those files with a 7.6 micron diameter bead from the October 2023 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning a tuple of the following arrays:\n",
      "(diam_bead, mass_bead, p0_bead, descrips, qpd_diag_mats, cant_bins_x, cant_bins_z)\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25335/545293291.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  laser_power = aggdat.agg_dict['mean_laser_power'][(diameters==7.6) & (descrips=='Oct 2023')]\n"
     ]
    }
   ],
   "source": [
    "# we only care about the first array returned in this case\n",
    "diameters,_,descrips,*_ = aggdat.get_parameter_arrays()\n",
    "\n",
    "# now use the array of diameters to index within agg_dict\n",
    "laser_power = aggdat.agg_dict['mean_laser_power'][(diameters==7.6) & (descrips=='Oct 2023')]\n",
    "print()\n",
    "print(laser_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing using the built-in method\n",
    "The `get_slice_indices()` method is designed as a user-friendly wrapper for this type of slicing. This method takes the following arguments:\n",
    "* `diam_bead`: a float or a list of floats containing the desired bead diameters in microns.\n",
    "* `descrips`: a string or list of strings containing the descriptions of the datasets chosen when the constructor was called.\n",
    "* `cant_x`: a range of mean cantilever x positions in microns to include, of the form `[lower,upper]`.\n",
    "* `cant_z`: same as above for the mean cantilever z positions.\n",
    "* `seis_veto`: a boolean that says whether or not data not passing the seismometer cut should be included.\n",
    "\n",
    "This method returns an array of the indices of files that pass the cuts. As an example, in the cell below we create separate index arrays for the Wilson data and the October 2023 data and require that all files from both pass the siesmometer cut. This array of indices is then used to compare the QPD x and y spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files from Wilson data:  100\n",
      "Number of files from Oct 2023 data:  96\n"
     ]
    }
   ],
   "source": [
    "# get arrays of indices\n",
    "indices_wilson = aggdat.get_slice_indices(descrip='Wilson')\n",
    "indices_2023 = aggdat.get_slice_indices(descrip='Oct 2023')\n",
    "print('Number of files from Wilson data: ',len(indices_wilson))\n",
    "print('Number of files from Oct 2023 data: ',len(indices_2023))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous file handling methods\n",
    "`AggregateData` objects can be saved and loaded using the `save_to_hdf5()` and `load_from_hdf5()` methods respectively. The only argument to both is the path to where the object should be/was saved. Since the loading method relies on the object having been saved in a particular format, it may not work for objects that were saved with a different version of the code. However, as objects are saved in HDF5 format, the file can be loaded using `h5py` and the datasets extracted based on the (hopefully self-explanatory) descriptors.\n",
    "\n",
    "Any number of `AggregateData` objects can be merged into one after the fact using the `merge_objects()` method with a list of objects as the argument, so that if more datasets need to be added the existing data does not need to be reprocessed. This should be called by constructing a new `AggregateData` object first and then calling the method to fill it with the existing objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clarkeh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
